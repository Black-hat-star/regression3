{
 "cells": [
  {
   "cell_type": "raw",
   "id": "00b6045e-418a-418e-a069-db5aa8533228",
   "metadata": {},
   "source": [
    "1)Ridge regularization also known as L2regularization is a linear regression which is used to handle the problem of multlicollinearity in multliple linear regression,in ridge regression a regularization term is added to the mean squared error.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "766f2927-7bc6-47a0-882e-fae5997b8ed4",
   "metadata": {},
   "source": [
    "2)Ridge regression's key assumption are :-\n",
    "i)Linearity-It assumes a linear relationship between the independent variables and the dependent variables.\n",
    "ii)The errors should be independent of each other.The error of one observation should not provide information about other errors.\n",
    "iii) it assumes that there is no perfect multicollinearity (i.e., no variable is a perfect linear combination of others)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac3e4023-10ce-4fd2-860a-221ebd718b26",
   "metadata": {},
   "source": [
    "3)The common method for selecting the tuning parametre in ridge regression is:-\n",
    "i)Grid search-Apredefined set of lambda is choosen and the model is trained and the evaluated for each lambda in the set.The lambda that gives the best performance on the validation set is selected.\n",
    "ii)K-Fold Cross-Validation: The dataset is divided into K folds. The model is trainedon K−1 folds and validated on the remaining fold. This process is repeated \n",
    "K times, and the average performance is computed for each λ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "efc715fe-114a-49eb-bb75-6a82074c3e58",
   "metadata": {},
   "source": [
    "4)Ridge regression penalizes the feature with by tending its cooficient to nearly 0 which reduces impact of the feature."
   ]
  },
  {
   "cell_type": "raw",
   "id": "971754ca-b3e4-4463-b5e2-9e4dd139c749",
   "metadata": {},
   "source": [
    "5)Ridge regression one of the fundamental objective is to tackle multliplecollinearity by adding a hyperparamtre term to mean squared error term."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0920162-34ef-4949-8986-85b1adb3cd88",
   "metadata": {},
   "source": [
    "6)Yes ridge regression can handle both categorical and continous independent variable by converting the categorical features into numerical features for conversion we can use OHE(one hot encoder)\n",
    "and Label encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8145bab0-ebbd-4242-84f2-ee3decaa6721",
   "metadata": {},
   "source": [
    "7)The cooficients in Ridge regression are shrunk towards zero due to the regularization term.The larger the value of lambda  the stronger the shrinkage \n",
    "Penalizizng of independent variables:Ridge regression helps to penalize the cooficients of less important variables making the model more sensitive towards outliers "
   ]
  },
  {
   "cell_type": "raw",
   "id": "119a9c19-5999-44bf-9d87-5fdeeef252d5",
   "metadata": {},
   "source": [
    "8)Yes ridge regression can be used for time series data analysis Ridge regression assumes that the relationship between variables is stationary over time.Stationary implies that the statistical properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4ec51-b7d0-45c2-80a3-3317d8330e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
