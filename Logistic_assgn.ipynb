{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0fe42483-4529-4480-80ad-cd587a0ed314",
   "metadata": {},
   "source": [
    "1)In linear regression it is sensitive to outliers where the best fit line changes due to it \n",
    "whereas in logistic regression we can handle this problem\n",
    "Linear regression is used for predicting a continuous outcome variable based on one or more predictor variables. The model assumes a linear relationship between the predictors and the response variable\n",
    "\n",
    "on the other hand, is used when the dependent variable is binary (i.e., it has only two possible outcomes, often coded as 0 or 1). The logistic regression model uses the logistic function to model the probability that the dependent variable belongs to a particular category. The logistic function, also known as the sigmoid function, has an S-shaped curve\n",
    "\n",
    "Logistic regression is more appropriate when dealing with binary classification problems. For example, predicting whether an email is spam (1) or not spam (0), determining whether a student will pass (1) or fail (0) an exam, or predicting whether a customer will purchase a product (1) or not (0)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "035812f6-703e-4409-b376-227d62e4d4d4",
   "metadata": {},
   "source": [
    "The cost function used in logistic regression is \n",
    "-ylogho(x))-(1-y)log(1-ho(x))  where y is the true value and ho(x) is the predicted value\n",
    "Optimizing the cost function is typically done using an optimization algorithm. Gradient Descent is a common optimization algorithm used for logistic regression. The idea is to iteratively update the parameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "931a0d56-c85d-4726-9e3b-665940d30b43",
   "metadata": {},
   "source": [
    "3)Regularization helps prevent overfitting by adding a penalty for large parameter values. This penalty discourages the model from fitting the training data too closely, reducing the likelihood of capturing noise and making the model more generalizable to new data. The choice of the regularization parameterλ) is crucial, as it determines the trade-off between fitting the data well and keeping the model simple. Cross-validation is often used to find an optimal value for λ that balances these competing objectives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d64404d8-24fb-485c-b5f4-d6b35e79f417",
   "metadata": {},
   "source": [
    "4)The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a classification model, particularly in the context of binary classification problems. It visualizes the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for different threshold values used to classify instances.\n",
    "True Positive Rate (Sensitivity):\n",
    "\n",
    "True Positive Rate (TPR) is the proportion of actual positive instances correctly predicted as positive by the model.\n",
    "TPR is calculated as True postive / (True Positives+FalseNegatives)\n",
    "False Positive Rate:\n",
    "\n",
    "False Positive Rate (FPR) is the proportion of actual negative instances incorrectly predicted as positive by the model.\n",
    "FPR is calculated as \n",
    "False Positives / (False Positives+TrueNegatives)\n",
    "False Positives+True Negatives\n",
    "False Positives\n",
    "\n",
    "\n",
    "Using ROC Curve for Logistic Regression:\n",
    "\n",
    "Model Evaluation: The ROC curve helps visualize how well a logistic regression model discriminates between the positive and negative classes across different decision thresholds.\n",
    "\n",
    "Threshold Selection: By examining the ROC curve, you can choose a threshold that balances sensitivity and specificity based on the specific needs of the problem. Some applications may require higher sensitivity, while others may prioritize specificity.\n",
    "\n",
    "Comparing Models: The ROC curve allows for the comparison of multiple models. The model with a higher AUC-ROC is generally considered to have better overall performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1bfd9a1-c08b-49ce-907d-8effe5bba126",
   "metadata": {},
   "source": [
    "5)Thwe techniques for feature selection in logistic regression are\n",
    "confusion matrix,accuracy,precesion,recall,F-beta score these all help in calculating model accuracy and helps in comaprison of accuracies of multliple model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "197b0387-8ca3-4ded-b8ca-348eef905b41",
   "metadata": {},
   "source": [
    "6)esampling Techniques:\n",
    "\n",
    "Undersampling: Reduce the size of the majority class by randomly removing instances. This helps balance the class distribution but may result in information loss.\n",
    "Oversampling: Increase the size of the minority class by replicating or generating new instances. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) create synthetic samples to balance the dataset.\n",
    "\n",
    "Different Class Weights:\n",
    "\n",
    "Adjust the class weights during model training to give more importance to the minority class. In logistic regression, this is often done by assigning higher weights to the minority class. Many machine learning libraries allow you to specify class weights in the model training process.\n",
    "Threshold Adjustment:\n",
    "\n",
    "By default, logistic regression predicts the class with a probability threshold of 0.5. Adjusting this threshold can be beneficial for imbalanced datasets. Increasing the threshold can increase specificity but may reduce sensitivity.\n",
    "\n",
    "Cost-sensitive Learning:\n",
    "\n",
    "Modify the cost function to penalize misclassifying the minority class more than the majority class. This can be incorporated into logistic regression by adjusting the loss function or using specific libraries that support cost-sensitive learning.\n",
    "Anomaly Detection Techniques:\n",
    "\n",
    "Treat the minority class as an anomaly and use anomaly detection techniques. This involves modeling the majority class and identifying instances that deviate from the norm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fa56de6-7c65-4392-bd32-b1dd106e9273",
   "metadata": {},
   "source": [
    "7)Remove one of the correlated variables: Identify and remove one of the variables causing multicollinearity.\n",
    "Combine correlated variables: Create a new variable as a combination (e.g., a weighted sum) of the correlated variables.\n",
    "Regularization: Use regularization techniques (L1 or L2 regularization) to penalize large coefficients and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465e5e7-8804-4260-93e4-9943b4020811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
