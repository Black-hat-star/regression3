{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d46347d1-f808-4c6f-948f-cf481fa2f087",
   "metadata": {},
   "source": [
    "1)Kernel Functions in SVMs:\n",
    "Kernel functions are mathematical functions that take as input the feature vectors of two data points and output a measure of similarity between them. In the context of SVMs, kernel functions are used to implicitly map the input data into a higher-dimensional space without explicitly computing the transformed feature vectors. This allows SVMs to handle non-linear decision boundaries.\n",
    "\n",
    "\n",
    "\n",
    "Polynomial Kernel:\n",
    "The polynomial kernel is a specific type of kernel function commonly used in SVMs. It is defined as:\n",
    "\n",
    "Relationship:\n",
    "Polynomial Kernel as a Special Case:\n",
    "\n",
    "The polynomial kernel is a specific instance of a more general concept of kernel functions.\n",
    "It represents a way to measure the similarity between data points by considering polynomial relationships.\n",
    "Generalization of Polynomial Functions:\n",
    "\n",
    "Polynomial kernels generalize the concept of polynomial functions to higher-dimensional spaces without explicitly computing the transformed feature vectors.\n",
    "Polynomial functions of different degrees can be expressed through the polynomial kernel by adjusting the parameterd.\n",
    "Flexibility in Representing Relationships:\n",
    "\n",
    "The choice of the degree d in the polynomial kernel provides flexibility in capturing different degrees of polynomial relationships between data points.\n",
    "Higher degrees allow the model to capture more complex non-linear patterns in the data.\n",
    "Unified Framework:\n",
    "\n",
    "Kernel functions, including the polynomial kernel, provide a unified framework for expressing various types of relationships between data points.\n",
    "Other kernel functions, such as radial basis function (RBF) or Gaussian kernel, capture different types of relationships beyond polynomial ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca964e53-9ec7-4c29-a9e4-f29f45b21b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "### 2)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# You can adjust the degree parameter to set the degree of the polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f91e2c94-5603-4a14-baf0-5613d2fdf2ae",
   "metadata": {},
   "source": [
    "3)n Support Vector Regression (SVR), the parameter epsilon (\n",
    "�\n",
    "ε) is a part of the algorithm that defines the width of the tube within which no penalty is associated with the training data points. This tube is often referred to as the epsilon-insensitive tube, and it plays a crucial role in determining which data points are considered support vectors.\n",
    "\n",
    "Epsilon-insensitive Tube:\n",
    "In SVR, the goal is to fit a hyperplane that lies within an epsilon-insensitive tube around the training data points. Data points within this tube do not contribute to the loss function, and only points outside the tube incur a penalty.\n",
    "\n",
    "Relationship Between Epsilon and Support Vectors:\n",
    "Smaller Epsilon (\n",
    "�\n",
    "ε):\n",
    "\n",
    "A smaller epsilon corresponds to a narrower epsilon-insensitive tube.\n",
    "When the tube is narrow, fewer data points are within the tube, and more data points become support vectors as they contribute to the loss function.\n",
    "Larger Epsilon (\n",
    "�\n",
    "ε):\n",
    "\n",
    "A larger epsilon corresponds to a wider epsilon-insensitive tube.\n",
    "With a wider tube, more data points may fall within it, leading to fewer data points considered as support vectors.\n",
    "Impact on Model Complexity:\n",
    "Smaller Epsilon:\n",
    "\n",
    "A smaller epsilon results in a more complex model that tries to fit the training data more closely.\n",
    "The model may be sensitive to noise in the training data.\n",
    "Larger Epsilon:\n",
    "\n",
    "A larger epsilon results in a simpler model that allows for more flexibility and tolerance to variations in the training data.\n",
    "The model focuses on capturing the overall trend rather than fitting individual data points.\n",
    "Practical Considerations:\n",
    "Tuning Epsilon:\n",
    "\n",
    "The choice of epsilon depends on the specific characteristics of the data and the desired trade-off between model complexity and generalization.\n",
    "Cross-validation or grid search can be used to find an appropriate value for epsilon based on the performance of the SVR model on validation data.\n",
    "Regularization Effect:\n",
    "\n",
    "Epsilon is one of the parameters that contribute to the regularization of the SVR model.\n",
    "Adjusting epsilon allows for controlling the balance between fitting the training data closely and maintaining generalization to unseen data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "53355faf-1978-470e-bc3c-b0af32c0a4a4",
   "metadata": {},
   "source": [
    "4)Kernel Function:\n",
    "\n",
    "Role: The kernel function determines the type of mapping applied to the input data. Common choices include linear, polynomial, and radial basis function (RBF or Gaussian).\n",
    "Effect: The kernel function affects the model's ability to capture complex relationships in the data. The RBF kernel is often a good default choice and works well for a wide range of problems.\n",
    "Example: If the relationship between input features and the target variable is highly nonlinear, using an RBF kernel might be beneficial.\n",
    "C Parameter:\n",
    "\n",
    "Role: The C parameter controls the trade-off between having a smooth decision boundary and fitting the training data points.\n",
    "Effect: A smaller C value makes the decision boundary smoother, potentially leading to a simpler model with fewer support vectors. A larger C value allows the model to fit the training data more closely.\n",
    "Example: If the training data contains outliers, using a smaller C value may help the model generalize better to unseen data. On the other hand, if the training data is noise-free, a larger C value might be suitable for closely fitting the data.\n",
    "Epsilon Parameter (ε):\n",
    "\n",
    "Role: The epsilon parameter determines the margin of tolerance for errors in the regression model.\n",
    "Effect: A smaller epsilon allows for a narrower margin and a tighter fit to the training data. A larger epsilon permits a wider margin, potentially improving the model's ability to generalize to unseen data.\n",
    "Example: If the training data has some level of noise, a larger epsilon may help the model ignore small fluctuations and focus on capturing the overall trend.\n",
    "Gamma Parameter:\n",
    "\n",
    "Role: The gamma parameter defines how far the influence of a single training example reaches.\n",
    "Effect: A small gamma value makes the influence of each example more widespread, leading to a smoother decision boundary. A large gamma value makes the influence more localized, capturing fine details of the training data.\n",
    "Example: If the dataset is large and diverse, a smaller gamma value might be suitable for generalization. For datasets with intricate patterns, a larger gamma value can help the model focus on local details.\n",
    "Parameter Tuning Tips:\n",
    "\n",
    "Use techniques like cross-validation to find the optimal values for C, epsilon, and gamma.\n",
    "Grid search or randomized search can be employed to explore multiple combinations of hyperparameters efficiently.\n",
    "Always validate the model on a separate validation set or through cross-validation to ensure it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e5ca99b-a580-4313-9081-bdafc57d5099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Best Hyperparameters: {'C': 0.1, 'degree': 2, 'kernel': 'linear'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the tuned classifier on the entire dataset\u001b[39;00m\n\u001b[1;32m     50\u001b[0m tuned_svc_classifier \u001b[38;5;241m=\u001b[39m SVC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n\u001b[0;32m---> 51\u001b[0m tuned_svc_classifier\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_scaled\u001b[49m, y)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Save the trained classifier to a file for future use\u001b[39;00m\n\u001b[1;32m     54\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(tuned_svc_classifier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtuned_svc_classifier.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "###5)\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib  # for saving the trained classifier\n",
    "\n",
    "# Load the dataset (I'm using the Iris dataset as an example)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc_classifier = SVC(**best_params)\n",
    "tuned_svc_classifier.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_svc_classifier, 'tuned_svc_classifier.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0819f-3db5-447f-9be2-05cda093d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
